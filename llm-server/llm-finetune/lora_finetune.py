# lora_finetune.py
import os
#os.environ["WANDB_PROJECT"] = "lora-finetune-demo"  # Optional: ä½¿ç”¨ wandb è®°å½•è®­ç»ƒ

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset, Dataset
import json
import torch
import warnings
import datetime

#transformers >= 4.37.0 åºŸå¼ƒäº†æ—§çš„æ¢¯åº¦æ£€æŸ¥ç‚¹è®¾ç½®æ–¹å¼_set_gradient_checkpointing() æ–¹æ³•ï¼ˆQwen å°±æ˜¯è¿™ä¹ˆåšçš„ï¼‰
warnings.filterwarnings("ignore", message="You are using an old version of the checkpointing format")
# -------------------------------
# 1. æ¨¡å‹ä¸ tokenizer åŠ è½½
# -------------------------------
model_path = "../Qwen/Qwen3-14B"  # å¯æ›¿æ¢ä¸ºä½ æƒ³å¾®è°ƒçš„æ¨¡å‹

#ä» Hugging Face çš„æ¨¡å‹ä»“åº“ä¸­åŠ è½½ä¸æŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹ï¼ˆmodel_pathï¼‰å¯¹åº”çš„åˆ†è¯å™¨ï¼ˆTokenizerï¼‰
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
#å°†åˆ†è¯å™¨ï¼ˆtokenizerï¼‰çš„å¡«å……æ ‡è®°ï¼ˆpad tokenï¼‰ è®¾ç½®ä¸ºä¸ç»“æŸæ ‡è®°ï¼ˆeos tokenï¼‰ ç›¸åŒ

# ğŸ”¥ å…³é”®ï¼šæ‰“å°åŸå§‹çŠ¶æ€
print(f"Original eos_token: {tokenizer.eos_token}, eos_token_id: {tokenizer.eos_token_id}")
print(f"Original pad_token: {tokenizer.pad_token}, pad_token_id: {tokenizer.pad_token_id}")

# âœ… ä½¿ç”¨ add_special_tokens çœŸæ­£è®¾ç½® pad_token
tokenizer.pad_token = '<ï½œendoftextï½œ>'
tokenizer.pad_token_id = 151643

# âœ… å†æ¬¡éªŒè¯
print(f"âœ… Final pad_token: {tokenizer.pad_token}")
print(f"âœ… Final pad_token_id: {tokenizer.pad_token_id}")
print(f"âœ… Final vocab size: {len(tokenizer)}")

tokenizer.padding_side = "right"

# æ˜¯å¦ä½¿ç”¨ 4-bit é‡åŒ– (QLoRA)
use_4bit = True

if use_4bit:
    from transformers import BitsAndBytesConfig
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ° GPU
        trust_remote_code=True
    )
    # ä¸ºé‡åŒ–æ¨¡å‹å‡†å¤‡ï¼šæ·»åŠ æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œæ¿€æ´»æ£€æŸ¥
    model = prepare_model_for_kbit_training(model)
else:
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )

# ğŸ‘‡ æ‰“å°æ‰€æœ‰åŒ…å« 'proj' çš„ nn.Linear å±‚åç§°
print("ğŸ” Finding projection layers in Qwen3:")
target_candidates = []
for name, module in model.named_modules():
    if 'proj' in name and isinstance(module, torch.nn.Linear):
        print(f"  {name}")
        target_candidates.append(name)

# å¯é€‰ï¼šæå–æœ€åä¸€çº§åç§°ï¼ˆå¦‚ q_proj, v_proj ç­‰ï¼‰
# ä¾‹å¦‚ï¼šä» 'model.layers.0.self_attn.q_proj' æå– 'q_proj'
base_names = list(set([name.split('.')[-1] for name in target_candidates]))
print(f"\nğŸ¯ Candidate target_modules: {base_names}")

# -------------------------------
# 2. åŠ è½½ä¸é¢„å¤„ç†æ•°æ®é›†
# -------------------------------
# ä½¿ç”¨ Alpaca é£æ ¼çš„æŒ‡ä»¤æ•°æ®é›†ï¼ˆç¤ºä¾‹ç”¨ 'tatsu-lab/alpaca'ï¼‰ï¼Œæ ¼å¼å¦‚ä¸‹
#{
#    "instruction": "è§£é‡Šä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„",
#    "input": "",  # æ— é¢å¤–è¾“å…¥æ—¶ä¸ºç©º
#    "output": "å¤©ç©ºå‘ˆç°è“è‰²æ˜¯å› ä¸ºç‘åˆ©æ•£å°„ç°è±¡..."
#}
#

# è¯»å–æœ¬åœ°çš„ JSON æ•°æ®
data_path = "alpaca_data.json"  # æ›¿æ¢ä¸ºä½ è‡ªå·±çš„æ•°æ®è·¯å¾„
with open(data_path, "r", encoding="utf-8") as f:
    train_datas  = json.load(f)

# å°†alpacaæ ¼å¼çš„æ•°æ®è½¬ä¸ºqwençš„chattemplateæ ¼å¼
def convert_format(data_list):
    converted_datas = []
    for item in data_list:
        # æ„å»º user çš„ content
        user_content = item["instruction"]
        if item["input"].strip():  # æ£€æŸ¥ input æ˜¯å¦éç©ºï¼ˆå»é™¤ç©ºæ ¼åï¼‰
            user_content = f"{item['instruction']}\n\n{item['input']}"
            # æˆ–è€…æ ¹æ®è¯­ä¹‰è°ƒæ•´é¡ºåºï¼Œæ¯”å¦‚ input æ˜¯ä¸»è¦æ–‡æœ¬æ—¶ï¼šf"{item['input']}\n\n{item['instruction']}"

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹"},
            {"role": "user", "content": user_content},
            {"role": "assistant", "content": item["output"]}
        ]
        converted_datas.append({"messages": messages})
    return converted_datas

# è°ƒç”¨è½¬æ¢å‡½æ•°
converted_datas = convert_format(train_datas)

def create_and_prepare_dataset(data_list):
    """
    å°†åŸå§‹æ•°æ®åˆ—è¡¨è½¬æ¢ä¸º Hugging Face Dataset æ ¼å¼ï¼Œå¹¶åº”ç”¨èŠå¤©æ¨¡æ¿ã€‚
    """
    def apply_chat_template(example):
        messages = example["messages"]
        # ä½¿ç”¨åˆ†è¯å™¨çš„ apply_chat_template æ–¹æ³•å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
        try:
            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        except Exception as e:
            print(f"Error applying chat template: {e}")
            prompt = "" # æˆ–è€…å¯ä»¥è·³è¿‡è¿™ä¸ªæ ·æœ¬
        print(f"æ‰“å° LoRA è®­ç»ƒæ•°æ®ã€‚ text: {prompt}")
        return { "text": prompt }

    # åˆ›å»º Dataset å¯¹è±¡
    raw_dataset = Dataset.from_list(data_list)

    # åº”ç”¨æ¨¡æ¿å‡½æ•°åˆ°æ•´ä¸ªæ•°æ®é›†
    processed_dataset = raw_dataset.map(apply_chat_template)

    return processed_dataset

# åº”ç”¨èŠå¤©æ¨¡æ¿
dataset = create_and_prepare_dataset(converted_datas)
print(f"ğŸš€  æ‰“å° LoRA è®­ç»ƒæ•°æ®ã€‚dataset:{dataset}")

# Tokenize å‡½æ•°
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding=False,
        max_length=512,
        truncation=True,
        return_tensors=None,  # è¿”å› Python listï¼Œç”± Trainer å¤„ç†
    )

# 3. å¤„ç†æ•°æ®é›†
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=[col for col in ["messages","text"] if col in dataset.column_names],
    num_proc=4
)
print(f"ğŸš€  æ‰“å° å¤„ç†åçš„æ•°æ®é›† tokenized_dataset :{tokenized_dataset}")

# æ•°æ®æ•´ç†å™¨ï¼ˆè‡ªåŠ¨å¤„ç† paddingï¼‰
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

# -------------------------------
# 3. é…ç½® LoRA
# -------------------------------
lora_config = LoraConfig(
    r=8,                        # LoRA ç§©
    lora_alpha=16,               # è¶…å‚
    # Qwen3 ä¸­ QKV æŠ•å½±çš„ç»Ÿä¸€å±‚["q_proj", "k_proj", "v_proj", "o_proj" ...]
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    #åœ¨ä½ç§©æ›´æ–°æ¨¡å—ä¸­å¼•å…¥ 10% çš„éšæœºä¸¢å¼ƒæ¦‚ç‡ï¼Œ
    #é¿å…æ¨¡å‹è¿‡åº¦ä¾èµ– LoRA æ–°å¢å‚æ•°æ‹Ÿåˆè®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°ï¼Œæé«˜å¯¹æœªè§è¿‡æ•°æ®çš„é€‚é…èƒ½åŠ›
    lora_dropout=0.1,
    #æŒ‡å®šä¸å¯¹æ¨¡å‹çš„åç½®å‚æ•°ï¼ˆbiasï¼‰è¿›è¡Œå¾®è°ƒæˆ–ä¿®æ”¹
    bias="none",
    task_type="CAUSAL_LM"        # å› æœè¯­è¨€å»ºæ¨¡
)

#å°†åŸå§‹é¢„è®­ç»ƒæ¨¡å‹ä¸ LoRAï¼ˆæˆ– QLoRAï¼‰é…ç½®ç»“åˆï¼Œç”Ÿæˆä¸€ä¸ªæ”¯æŒå‚æ•°é«˜æ•ˆå¾®è°ƒçš„ PEFT æ¨¡å‹
print(f"\nğŸ¯  å°†åŸå§‹é¢„è®­ç»ƒæ¨¡å‹ä¸ LoRAï¼ˆæˆ– QLoRAï¼‰é…ç½®ç»“åˆï¼è¿™ä¸ªè¿‡ç¨‹æ¯”è¾ƒè€—æ—¶ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼")
start_time = datetime.datetime.now()
model = get_peft_model(model, lora_config)
end_time = datetime.datetime.now()
cos_time = (end_time - start_time).seconds
print(f"åŸå§‹é¢„è®­ç»ƒæ¨¡å‹ä¸ LoRAï¼ˆæˆ– QLoRAï¼‰é…ç½®ç»“åˆå®Œæˆã€‚è€—æ—¶ï¼š{cos_time} ç§’ã€‚")
model.print_trainable_parameters()  # æŸ¥çœ‹å¯è®­ç»ƒå‚æ•°é‡ï¼ˆé€šå¸¸ <1%ï¼‰

# -------------------------------
# 4. é…ç½®è®­ç»ƒå‚æ•°
# -------------------------------
training_args = TrainingArguments(
    output_dir="./lora-alpaca-qwen3",  # æ¨¡å‹è®­ç»ƒç»“æœï¼ˆ checkpointã€æ—¥å¿—ç­‰ ï¼‰çš„ä¿å­˜è·¯å¾„
    num_train_epochs=200,  # è®­ç»ƒçš„æ€»è½®æ•°ï¼Œå³å®Œæ•´éå†è®­ç»ƒé›†çš„æ¬¡æ•°
    per_device_train_batch_size=4,  # æ¯ä¸ªè®¾å¤‡ï¼ˆå¦‚å•å¼ GPUï¼‰ä¸Šçš„è®­ç»ƒæ‰¹æ¬¡å¤§å°
    gradient_accumulation_steps=4,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œæ¯ç´¯ç§¯4ä¸ªæ‰¹æ¬¡åå†æ›´æ–°ä¸€æ¬¡å‚æ•°ï¼ˆå˜ç›¸å¢å¤§æ€»batch sizeï¼‰
    learning_rate=2e-4,  # å­¦ä¹ ç‡ï¼ŒLoRAå¾®è°ƒå¸¸ç”¨2e-4 ~ 5e-4
    logging_steps=10,  # æ¯è®­ç»ƒ10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—ï¼ˆå¦‚æŸå¤±å€¼ï¼‰
    save_steps=100,  # æ¯è®­ç»ƒ500æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹ checkpoint
    save_total_limit=2,  # æœ€å¤šä¿ç•™2ä¸ªæœ€æ–°çš„æ¨¡å‹ checkpointï¼Œé¿å…å ç”¨è¿‡å¤šå­˜å‚¨ç©ºé—´
    fp16=False,  # ä¸ä½¿ç”¨FP16æ··åˆç²¾åº¦è®­ç»ƒ
    bf16=torch.cuda.is_bf16_supported(),  # è‹¥GPUæ”¯æŒBF16ç²¾åº¦åˆ™å¯ç”¨ï¼ˆæ¯”FP16æ›´ç¨³å®šï¼Œæ˜¾å­˜å ç”¨ç›¸ä¼¼ï¼‰
    optim="paged_adamw_8bit",  # ä½¿ç”¨8ä½é‡åŒ–çš„PagedAdamWä¼˜åŒ–å™¨ï¼ˆé…åˆbitsandbytesåº“ï¼Œå‡å°‘æ˜¾å­˜å ç”¨ï¼‰
    lr_scheduler_type="cosine",  # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼Œé‡‡ç”¨ä½™å¼¦é€€ç«ç­–ç•¥ï¼ˆè®­ç»ƒåæœŸè‡ªåŠ¨é™ä½å­¦ä¹ ç‡ï¼‰
    warmup_ratio=0.03,  # å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹ï¼Œå‰3%çš„è®­ç»ƒæ­¥æ•°é€æ¸å°†å­¦ä¹ ç‡ä»0æå‡åˆ°è®¾å®šå€¼ï¼ˆç¨³å®šè®­ç»ƒåˆæœŸï¼‰
    #report_to="wandb",  # è®­ç»ƒæ—¥å¿—æŠ¥å‘Šåˆ°Weights & Biaseså¹³å°ï¼ˆéœ€æå‰å®‰è£…wandbå¹¶ç™»å½•ï¼‰
    disable_tqdm=False,  # ä¸ç¦ç”¨tqdmè¿›åº¦æ¡ï¼ˆæ˜¾ç¤ºè®­ç»ƒè¿›åº¦ï¼‰
    gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆç‰ºç‰²å°‘é‡è®¡ç®—é€Ÿåº¦ï¼Œå¤§å¹…å‡å°‘æ˜¾å­˜å ç”¨ï¼‰
)
# -------------------------------
# 5. åˆ›å»º Trainer å¹¶å¼€å§‹è®­ç»ƒ
# -------------------------------
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,# <<< è¿™é‡Œä¼ å…¥äº†æ•°æ®é›†ï¼
    data_collator=data_collator,
    tokenizer=tokenizer,
)

print("ğŸš€ å¼€å§‹ LoRA å¾®è°ƒ...")
trainer.train()

# -------------------------------
# 6. ä¿å­˜ LoRA é€‚é…å™¨
# -------------------------------
model.save_pretrained("lora-alpaca-qwen3-finetuned")
tokenizer.save_pretrained("lora-alpaca-qwen3-finetuned")

print("âœ… LoRA å¾®è°ƒå®Œæˆï¼Œé€‚é…å™¨å·²ä¿å­˜åˆ° 'lora-alpaca-qwen3-finetuned'")
